{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "import visdom\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "from model.multi_scale_ori import *\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from PIL import Image\n",
    "from utils import *\n",
    "from preprocessing import pre\n",
    "from config import cfg\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, logits=False, reduce=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.logits = logits\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        if self.logits:\n",
    "            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce=False)\n",
    "        else:\n",
    "            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce=False)\n",
    "        \n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "\n",
    "        if self.reduce:\n",
    "            return torch.mean(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    vis = visdom.Visdom(env=\"resnet1D_focal_loss_2\", port=8097)\n",
    "    \n",
    "    data = pre(train_data_path=cfg.train_data_path, \n",
    "           test_data_path=cfg.test_data_path,\n",
    "           y_train_path=cfg.train_label, \n",
    "           y_val_path=cfg.test_label,\n",
    "           batch_size=cfg.batch_size, \n",
    "           aspect_ratio=cfg.aspect_ratio)\n",
    "    \n",
    "    #clear gpu_cache\n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "    #set parameter \n",
    "    #class　\n",
    "    n_class = cfg.n_class\n",
    "\n",
    "    # Number of epochs to train for\n",
    "    num_epochs = cfg.num_epochs\n",
    "\n",
    "    # Flag for feature extracting. When False, we finetune the whole model,\n",
    "    # when True we only update the reshaped layer params\n",
    "    feature_extract = False\n",
    "\n",
    "    #use gpu\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #learning rate \n",
    "    lr=cfg.learn_rate\n",
    "    msresnet = MSResNet(input_channel=cfg.channel, layers=cfg.resnet_layer, num_classes=n_class)\n",
    "\n",
    "    # use GPU\n",
    "    msresnet.to(device)\n",
    "\n",
    "    #weight decay\n",
    "    weight_decay=cfg.weight_decay\n",
    "    \n",
    "    params_to_update = msresnet.parameters()\n",
    "\n",
    "    #Observe that all parameters are being optimized\n",
    "    optimizer = torch.optim.Adam(msresnet.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 100, 150, 200, 250, 300], gamma=0.1)\n",
    "    criterion = FocalLoss(logits=True)\n",
    "     \n",
    "    #train_model\n",
    "    currunt_turn = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "    \n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 100)\n",
    "     \n",
    "        for epoch_i, (inputs_nonacc, labels_nonacc) in enumerate(data.train_nonacc_dataloader):\n",
    "        \n",
    "            msresnet.train() \n",
    "            currunt_turn += 1\n",
    "        \n",
    "            #sampling_acc, labels_acc\n",
    "            inputs_acc, labels_acc = next(iter(data.train_acc_dataloader))\n",
    "        \n",
    "            #concat_sampling \n",
    "            inputs = torch.cat([inputs_nonacc, inputs_acc], axis=0) \n",
    "            labels = torch.cat([labels_nonacc, labels_acc], axis=0) \n",
    "        \n",
    "            #shuffle inputs and labels \n",
    "            shuffled_index = torch.randperm(cfg.batch_size + cfg.aspect_ratio * cfg.batch_size)\n",
    "            inputs = inputs[shuffled_index]\n",
    "            labels = labels[shuffled_index]\n",
    "        \n",
    "            #use GPU\n",
    "            inputs, labels = Variable(inputs.type(torch.FloatTensor).cuda()), Variable(labels.float().cuda())\n",
    "        \n",
    "            #init optimizer\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #print(inputs.shape)\n",
    "            #zeo_padd =  torch.zeros(200, 735, 3).cuda()\n",
    "            #inputs = torch.cat([zeo_padd, inputs], dim=1)\n",
    "            #print(inputs.shape)\n",
    "            \n",
    "            #print(inputs.shape)\n",
    "            p1d = (0,0,0,735) # pad last dim by 1 on each side\n",
    "            inputs = F.pad(inputs, p1d, \"constant\", 0) \n",
    "            #print(inputs.shape)\n",
    "            \n",
    "            #calc output\n",
    "            #outputs, _ = msresnet(inputs)\n",
    "            outputs, _ = msresnet(inputs.permute(0, 2, 1))\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "            #calc_pred_label\n",
    "            outputs = torch.sigmoid(outputs)\n",
    "            tag_one= torch.ones_like(outputs)\n",
    "            tag_zero= torch.zeros_like(outputs)\n",
    "            preds = torch.where(outputs >= 0.5, tag_one, tag_zero)\n",
    "        \n",
    "            preds = preds.data.cpu().numpy()\n",
    "            labels = labels.data.cpu().numpy()\n",
    "            \n",
    "            #calc f1 scores\n",
    "            precision = precision_score(labels, preds)\n",
    "            recall = recall_score(labels, preds)\n",
    "            f1 = f1_score(labels, preds)\n",
    "        \n",
    "            visulization(vis, ptype= \"line\",X=currunt_turn, Y=f1, win_name=\"f1\")       \n",
    "            visulization(vis, ptype= \"line\",X=currunt_turn, Y=loss.item(), win_name=\"loss\")\n",
    "        \n",
    "            #backward \n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "        \n",
    "            if epoch_i % 200 == 0: \n",
    "                dt_now = datetime.datetime.now()\n",
    "                #torch.save(msresnet.state_dict(), cfg.training_model_path + \"model_save/model_{}_{}.pth\".format(str(epoch) + \"_\" + str(epoch_i), str(dt_now.month) + \"_\" + str(dt_now.day)))\n",
    "                outputs_val_all, preds_val_all, labels_val_all, loss_val_all = cal_train_data(msresnet=msresnet, dataloader=data.test_dataloader, criterion=criterion)\n",
    "                outputs_train_all, preds_train_all, labels_train_all, loss_train_all = cal_train_data(msresnet=msresnet, dataloader=data.train_dataloader, criterion=criterion)\n",
    "            \n",
    "                loss_mean_test, f1_val = eval_metric(loss_val_all, labels_val_all, preds_val_all, epoch, epoch_i, phaze=\"test\")\n",
    "                loss_mean_train, f1_train = eval_metric(loss_train_all, labels_train_all, preds_train_all, epoch, epoch_i, phaze=\"train\")\n",
    "            \n",
    "                visulization(vis, ptype= \"line\",X=currunt_turn, Y=loss_mean_test, win_name=\"loss_test\")\n",
    "                visulization(vis, ptype= \"line\",X=currunt_turn, Y=f1_val, win_name=\"f1_test\")\n",
    "                visulization(vis, ptype= \"line\",X=currunt_turn, Y=loss_mean_train, win_name=\"loss_train\")\n",
    "                visulization(vis, ptype= \"line\",X=currunt_turn, Y=f1_train, win_name=\"f1_train\")\n",
    "        \n",
    "                plot_confusion_matrix(outputs_val_all, labels_val_all, fig_title=str(epoch) + \"_\" + str(epoch_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_shape: (67682, 750, 3)\n",
      "test_data_shape: (16881, 750, 3)\n",
      "y_train_shape: 3305\n",
      "y_val_shape: 827\n",
      "Epoch 0/1\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-7c4651bfcc30>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mdt_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0;31m#torch.save(msresnet.state_dict(), cfg.training_model_path + \"model_save/model_{}_{}.pth\".format(str(epoch) + \"_\" + str(epoch_i), str(dt_now.month) + \"_\" + str(dt_now.day)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0moutputs_val_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_val_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_val_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsresnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m                 \u001b[0moutputs_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_train_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcal_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsresnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/accident_detection/src_code/train/utils.py\u001b[0m in \u001b[0;36mcal_train_data\u001b[0;34m(msresnet, dataloader, criterion)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mpreds_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_val_epoch\u001b[0m \u001b[0;34m>=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_one_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_zero_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0moutputs_val_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs_val_epoch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mpreds_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mlabels_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 結果検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.to_pickle(testdata_compare_scsk, \"./model_result/demodata.pkl\")\n",
    "pd.to_pickle(y_val_compare_scsk, \"./model_result/demodata_y.pkl\")\n",
    "pd.to_pickle(selected_id, \"./model_result/noacc_id.pkl\")\n",
    "pd.to_pickle(test_acc_index, \"./model_result/acc_id.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_error(thr): \n",
    "    return np.where((np.array(labels_val_all) != np.array(preds_val_all)) & (np.array(outputs_val_all) <= thr))\n",
    "\n",
    "def find_index_error_bt_prob(thr, thr2): \n",
    "    return np.where((np.array(labels_val_all) != np.array(preds_val_all)) &\n",
    "                    (np.array(outputs_val_all) >= thr) &\n",
    "                    (np.array(outputs_val_all) <= thr2))\n",
    "\n",
    "index_error_001, _ = find_index_error(0.01)\n",
    "index_error_090, _ = find_index_error_bt_prob(0.9, 1)\n",
    "index_error_0506, _ = find_index_error_bt_prob(0.5, 0.6)\n",
    "index_error_0405, _ = find_index_error_bt_prob(0.4, 0.5)\n",
    "\n",
    "pd.to_pickle(index_error_001, \"./model_result/index_test_error_001.pkl\")\n",
    "pd.to_pickle(index_error_090, \"./model_result/index_test_error_090.pkl\")\n",
    "pd.to_pickle(index_error_0506, \"./model_result/index_test_error_0506.pkl\")\n",
    "pd.to_pickle(index_error_0405, \"./model_result/index_test_error_0405.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#間違ったデータを主力\n",
    "#index_test_error, _= np.where(np.array(labels_val_all) != np.array(preds_val_all))\n",
    "#index_train_error, _= np.where(np.array(labels_train_all) != np.array(preds_train_all))\n",
    "\n",
    "#間違ったデータのインデックスを出力\n",
    "#pd.to_pickle(index_error, \"./model_result/index_train_error.pkl\")\n",
    "#pd.to_pickle(index_error, \"./model_result/index_test_error.pkl\")\n",
    "\n",
    "#予測した確率を出力\n",
    "pd.to_pickle(outputs_train_all, \"./model_result/output_train_1.pkl\")\n",
    "\n",
    "#予測した確率を出力\n",
    "pd.to_pickle(outputs_val_all, \"./model_result/output_val_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeo_padd =  torch.zeros(200, 735, 3).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([zeo_padd, t4d.cuda()], dim=1)[0,: ,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4d = torch.ones(200, 15, 3)\n",
    "p1d = (0,0,0,735) # pad last dim by 1 on each side\n",
    "out = F.pad(t4d, p1d, \"constant\", 0)  # effectively zero padding\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
